{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7fXCyFHPr3X"
      },
      "source": [
        "# CIS 5450 Project: Difficulty Topics\n",
        "**Group Members:**\n",
        "* **Amogh Channashetti**\n",
        "* **Binoy Patel**\n",
        "* **Yohan Vergis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1jBlYLxQcNV"
      },
      "source": [
        "## **Topic 1: Imbalanced Data**\n",
        "[Hyperlink](https://colab.research.google.com/drive/1yPnYThJtgEq3xZUUYbIjLBBkydeEufLu?authuser=1#scrollTo=JhyzvkwG91FY\n",
        "  )\n",
        "### **Why we used this concept**\n",
        "Our target variable, 'is_hit', was extremely imbalanced. Only ~3.5% of tracks in the dataset qualify as hits. Hits are defined as popularity >= 70.\n",
        "\n",
        "The 28:1 imbalance makes the model favor non-hits. This gives high accuracy but poor performance on detecting hits.\n",
        "\n",
        "The goal of this analysis is to identify the characteristics of hit songs, so we needed to ensure the model could meaningfully learn patterns associated with hits. Handling imbalanced data is critical to avoid biased models and to ensure the classifier is evaluated fairly.\n",
        "\n",
        "### **How we implemented it**\n",
        "We applied three imbalance-handling strategies:\n",
        "\n",
        "1. **Class Weights (Baseline Logistic Regression)**  \n",
        "   - Applied `class_weight=\"balanced\"` to increase penalty on misclassified hits.\n",
        "\n",
        "2. **SMOTE Oversampling**  \n",
        "   - Used `SMOTE()` inside an `ImbPipeline` with scaling and logistic regression.  \n",
        "   - Used synthesized samples to increase the number of hit-song instances.\n",
        "3. **Random Undersampling**\n",
        "   - Reduced the majority class size to match hits more closely.\n",
        "   - Helps model focus on minority patterns at the cost of losing some data.\n",
        "\n",
        "### **Results & Interpretation**\n",
        "Across both models, feature importance was **highly consistent**:\n",
        "\n",
        "- SMOTE increased recall for hits. This shows the model can better identify minority samples.\n",
        "- Undersampling produced more balanced results, though at the cost of slightly less precision.\n",
        "- Class-weighted logistic regression provided a solid baseline, but its precision remained low.\n",
        "- These results demonstrated that resampling meaningfully changes model behavior. Imbalance handling is essential for interpreting model capability on rare hit songs.\n",
        "\n",
        "The insights from this analysis shaped our final model selection and highlighted the difficulty of predicting hits based solely on audio features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUfPpvutQyA3"
      },
      "source": [
        "## **Topic 2: Ensemble Models**\n",
        "[Hyperlink](https://colab.research.google.com/drive/1yPnYThJtgEq3xZUUYbIjLBBkydeEufLu?authuser=1#scrollTo=2U7snb1f4cK4&line=8&uniqifier=1)\n",
        "### **Why we used this concept**\n",
        "Predicting hit songs involves complex, nonlinear relationships between audio features such as energy, loudness, danceability, and acousticness. As a result, Linear models such logistic regression can't capture these interactions well and showed weak performance in our baseline tests.\n",
        "\n",
        "Ensemble models, particularly Random Forest and Gradient Boosting, are strong choices for noisy tabular data because they can learn nonlinear structures in the features. Using ensembles allowed us to improve predictive performance, capture richer feature interactions, and compare model behavior across different learning paradigms.\n",
        "\n",
        "### **How we implemented it**\n",
        "We trained and evaluated two esnsemble classifiers:\n",
        "\n",
        "1. **Random Forest Classifier**\n",
        "   - Tuned key params: number of trees, depth, and sample splits\n",
        "   - Calculated performance using ROC-AUC, PR-AUC, precision, and recall\n",
        "   - Extracted feature importances\n",
        "2. **Gradient Boosting Classifier** (baseline + tuned)\n",
        "   - Implemented a standard gradient boosting model\n",
        "   - Improved it later with hyperparameter tuning\n",
        "   - Evaluated using the same metrics for a fair comparison\n",
        "\n",
        "\n",
        "### **Results & Interpretation**\n",
        "\n",
        "\n",
        "- Random Forest achieved the best overall performance. This was especially the case in PR-AUC, indicating strong ranking capability for rare hits.  \n",
        "- Tuned Gradient Boosting improved significantly over its baseline model. This showed the effect of hyperparameter optimization.\n",
        "- Ensemble methods consistently outperformed logistic regression.\n",
        "\n",
        "This proved that Audio-based hit prediction requires nonlinear modeling capacity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMWIG0EXQ3sa"
      },
      "source": [
        "## **Topic 3: Hyperparameter Tuning** (RandomizedSearchCV on Gradient Boosting)\n",
        "[Hyperlink](https://colab.research.google.com/drive/1yPnYThJtgEq3xZUUYbIjLBBkydeEufLu?authuser=1#scrollTo=wMYAKQy5BBGx&line=5&uniqifier=1)\n",
        "### **Why we used this concept**\n",
        "Gradient Boosting is a strong model but highly sensitive to hyperparameters like learning rate, tree depth, and number of estimators. Our baseline model underperformed, indicating that it wasn't making good use of the available features. To achieve better performance and avoid underfitting/overfitting, we used RandomizedSearchCV, which efficiently explores a wide parameter space without the computational cost of grid search.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **How we implemented it**\n",
        "We defined a search space including:\n",
        "\n",
        "- `learning_rate`\n",
        "- `max_depth`\n",
        "- `n_estimators`\n",
        "- `subsample`\n",
        "\n",
        "RandomizedSearchCV was run with cross-validation using ROC-AUC as the scoring metric. Using the optimal parameters, we rebuilt the Gradient Boosting model and tested it on the same evaluation set as the rest.\n",
        "\n",
        "\n",
        "### **Results & Interpretation**\n",
        "The tuned model significantly outperformed the baseline Gradient Boosting model. Its higher ROC-AUC and improved PR-AUC are especially valuable for imbalanced hit prediction.\n",
        "\n",
        "Tuning allowed the model to capture more complex nonlinear interactions between audio features. This reduced the gap with Random Forest and showed that hyperparameter optimization makes a real difference for ensemble classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x01NPz5efJA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
